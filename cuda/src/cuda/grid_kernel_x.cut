// [2023/07/27]
// NOTE: Reorder the laucher and kernel so that kernel is before the launcher. This 
//       is because with C++17, the compiler would complain if otherwise and without
//       a forward (template) declaration. /* With C++20, it seems that we can use 
//       the launcher first then kernel order without a forward declaration.


template<bool bool_extra=false>
__global__ void cuda_interior_x ( cuda_Struct_Grid drvt_grid , cuda_Struct_Grid soln_grid ,
                                  cuda_run_time_vector_shallow_copy <ns_type::cuda_precision> stencil_dt_dx ,
                                  cuda_run_time_vector_shallow_copy <int> stencil_shift ,
                                  int INT_bound_BGN_x ,
                                  ns_type::cuda_precision * S ,  // S is from pntr grid
                                  ns_type::cuda_precision * P ,
                                  ns_type::cuda_precision * P_extra ,
                                  ns_type::cuda_precision * R ,
                                  ns_type::cuda_precision * R_extra )
{
    // int N_block  =  gridDim.x;   // Not needed because we don't loop through the (simulation) gridLines (one threadblock each gridLine)
    int N_thread = blockDim.x;

    int ind_block  =  blockIdx.x;
    int ind_thread = threadIdx.x;

    // [2022/07/18]
    // NOTE: One threadBlock is assigned to one gridLine; when calling this 
    //       kernel, the first argument in <<< >>> should be specified as 
    //       drvt_grid.interior_END_x - drvt_grid.interior_BGN_x, i.e., the
    //       total number of gridLines this kernel is responsible for.

    int ix = ind_block + INT_bound_BGN_x;
    {
        for ( int iy = ind_thread; iy < drvt_grid.G_size_y; iy += N_thread )  // iy goes through the grid index on drvt grid
        {    
            int i_d = ix * drvt_grid.Ly_pad + iy;
            int i_s = ix * soln_grid.Ly_pad + iy;

            ns_type::cuda_precision v_d = 0.;
    
            for ( int ind_stencil=0; ind_stencil<stencil_dt_dx.length; ind_stencil++ )
                { v_d += S[ i_s + stencil_shift( ind_stencil ) * soln_grid.Ly_pad ] * stencil_dt_dx( ind_stencil ); }

            drvt_to_rths <bool_extra> ( v_d, i_d, P, P_extra, R, R_extra );
            // [2023/07/18]
            // NOTE: "drvt_to_rths ()" should be called at the same level where "i_d" is calculated.
        }
    }

} // cuda_interior_x_I ()


template<bool bool_extra>
void cuda_Class_Grid::kernel_launch_cuda_interior_x ( int N_block , int N_thread )
{
    ns_type::cuda_precision * S = this->Map_pntr_soln.at('x')->ptr;

    ns_type::cuda_precision * R       = this->Vec_rths.at(0).ptr;
    ns_type::cuda_precision * P       = this->Vec_prmt.at(0).ptr;

    ns_type::cuda_precision * R_extra = nullptr;
    ns_type::cuda_precision * P_extra = nullptr;

    if ( bool_extra ) 
    {
        // [2023/07/17]
        // NOTE: It would be great if we could "static assert" here to check which grid is calling.
        //       Even better, if we have a member variable "bool_normal" of the grid class that is 
        //       available at compile time, we could use that variable for the "if test" instead. 
        //       Moreover, instead of having 'y' as part of the function name, maybe we can change
        //       it to a template parameter.

        R       = this->Vec_rths.at(1).ptr;  // 0 corresponds to 'y'
        R_extra = this->Vec_rths.at(0).ptr;  // 1 corresponds to 'x', which needs extra work

        P       = this->Vec_prmt.at(0).ptr;  // lambda
        P_extra = this->Vec_prmt.at(1).ptr;  // 2 * mu
    }


    // ---- loop bounds
    const int INT_bound_BGN_x = this->Map_interior_BGN.at('x');
    const int INT_bound_END_x = this->Map_interior_END.at('x');

    cuda_Struct_Grid & drvt_grid =                  this->struct_grid;
    cuda_Struct_Grid & soln_grid = Map_pntr_grid.at('x')->struct_grid;

    auto stencil_dt_dx    = this->stencil_dt_dx_shallow_copy;
    auto stencil_shift = this->Map_stencil_shift_shallow_copy.at('x');

    if ( N_block  == -1 ) { N_block  = INT_bound_END_x - INT_bound_BGN_x; }
    if ( N_thread == -1 ) { N_thread = 32; }

    cudaStream_t stream = bool_stream ? this->stream_dx_I : 0;

    cuda_interior_x <bool_extra> <<< N_block , N_thread , 0 , stream >>> 
        ( drvt_grid , soln_grid , stencil_dt_dx , stencil_shift , INT_bound_BGN_x , S , P , P_extra , R , R_extra );
}



template<bool bool_extra=false>
__global__ void cuda_boundary_x ( cuda_Struct_Grid drvt_grid , cuda_Struct_Grid soln_grid , 
                                  cuda_run_time_matrix_shallow_copy<ns_type::cuda_precision> D_bdry ,
                                  int row_BGN , int col_BGN ,    // row for x ; col for w
                                  ns_type::cuda_precision * S ,  // S is from pntr grid
                                  ns_type::cuda_precision * P ,
                                  ns_type::cuda_precision * P_extra ,
                                  ns_type::cuda_precision * R ,
                                  ns_type::cuda_precision * R_extra )
{
    int N_thread = blockDim.x;

    int ind_block  =  blockIdx.x;
    int ind_thread = threadIdx.x;

    int ix = ind_block + row_BGN;
    for ( int iy = ind_thread; iy < drvt_grid.G_size_y; iy += N_thread )  // iy goes through the grid index on drvt grid
    {
        int i_d = ix * drvt_grid.Ly_pad + iy;

        ns_type::cuda_precision v_d = 0.;
    
        for ( int iw = col_BGN; iw < col_BGN + D_bdry.cols; iw++ )  // iw goes through the grid index on soln grid
        {
            int i_s = iw * soln_grid.Ly_pad + iy;
            v_d += S[ i_s ] * D_bdry( ix - row_BGN , iw - col_BGN );
        }

        drvt_to_rths <bool_extra> ( v_d, i_d, P, P_extra, R, R_extra );
        // [2023/07/18]
        // NOTE: "drvt_to_rths ()" should be called at the same level where "i_d" is calculated.
    }
} // cuda_boundary_x ()


template<bool bool_extra>
void cuda_Class_Grid::kernel_launch_cuda_boundary_x ( int N_block , int N_thread , char c_LR )
{
    ns_type::cuda_precision * S = this->Map_pntr_soln.at('x')->ptr;

    ns_type::cuda_precision * R       = this->Vec_rths.at(0).ptr;
    ns_type::cuda_precision * P       = this->Vec_prmt.at(0).ptr;

    ns_type::cuda_precision * R_extra = nullptr;
    ns_type::cuda_precision * P_extra = nullptr;

    if ( bool_extra ) 
    {
        // [2023/07/17]
        // NOTE: It would be great if we could "static assert" here to check which grid is calling.
        //       Even better, if we have a member variable "bool_normal" of the grid class that is 
        //       available at compile time, we could use that variable for the "if test" instead. 
        //       Moreover, instead of having 'y' as part of the function name, maybe we can change
        //       it to a template parameter.

        R       = this->Vec_rths.at(1).ptr;  // 0 corresponds to 'y'
        R_extra = this->Vec_rths.at(0).ptr;  // 1 corresponds to 'x', which needs extra work

        P       = this->Vec_prmt.at(0).ptr;  // lambda
        P_extra = this->Vec_prmt.at(1).ptr;  // 2 * mu
    }

    cuda_Struct_Grid & drvt_grid =                  this->struct_grid;
    cuda_Struct_Grid & soln_grid = Map_pntr_grid.at('x')->struct_grid;

    auto D_bdry = this->Map_D_bdry_shallow_copy.at( {'x',c_LR} );
    // Question: should/can we use auto & D_bdry ?

    int row_BGN = -1<<30; 
    int col_BGN = -1<<30; 

    if ( c_LR == 'L' ) { row_BGN = 0; } 
    if ( c_LR == 'L' ) { col_BGN = 0; } 

    if ( c_LR == 'R' ) { row_BGN = drvt_grid.G_size_x - D_bdry.rows; }
    if ( c_LR == 'R' ) { col_BGN = soln_grid.G_size_x - D_bdry.cols; }

    if ( N_block  == -1 ) { N_block  = D_bdry.rows; }
    if ( N_thread == -1 ) { N_thread = 32;          }


    cudaStream_t stream = 0;  // 0 is the default stream
    if ( bool_stream ) { stream = c_LR == 'L' ? this->stream_dx_L : this->stream_dx_R; }

    cuda_boundary_x <bool_extra> <<< N_block , N_thread , 0 , stream >>> 
        ( drvt_grid , soln_grid , D_bdry , row_BGN , col_BGN , S , P , P_extra , R , R_extra );
}


template<bool bool_extra=false>
__global__ void cuda_secure_bdry_x ( cuda_Struct_Grid soln_grid , 
                                     cuda_run_time_vector_shallow_copy<ns_type::cuda_precision> projection , 
                                     int projection_BGN , // 0 for L ; soln_grid.G_size_x - projection.length for R
                                     cuda_Struct_Grid drvt_grid , 
                                     cuda_run_time_vector_shallow_copy<ns_type::cuda_precision> A_inv_projection , 
                                     int A_inv_projection_BGN ,     // 0 for L ; drvt_grid.G_size_x - A_inv_projection.length for R
                                     ns_type::cuda_precision eta ,  // 1 for L ; -1 for R
                                     ns_type::cuda_precision * S ,  // S is from pntr grid
                                     ns_type::cuda_precision * P ,
                                     ns_type::cuda_precision * P_extra ,
                                     ns_type::cuda_precision * R ,
                                     ns_type::cuda_precision * R_extra , 
                                     ns_type::cuda_precision * buffer )
{
    // int N_block  =  gridDim.x;    // Not needed because we don't loop through the (simulation) gridLines (one threadblock each gridLine)
    int N_thread = blockDim.x;

    // int ind_block  =  blockIdx.x; // Not used because there is only 1 gridLine (we are projecting onto a "plane")
    int ind_thread = threadIdx.x;

    for ( int iy = ind_thread; iy < soln_grid.G_size_y; iy += N_thread )  // iy goes through the grid index on soln_grid (same as drvt grid)
    {
        const int i_p = iy;
        buffer[ i_p ] =  0;

        for ( int iw = projection_BGN; iw < projection_BGN + projection.length; iw++ )  // iw goes through the grid index on soln grid
        {
            int i_s = iw * soln_grid.Ly_pad + iy;
            buffer[ i_p ] += projection( iw - projection_BGN ) * S[ i_s ];
        }
    }

    for ( int iy = ind_thread; iy < drvt_grid.G_size_y; iy += N_thread )  // iy goes through the grid index on drvt_grid (same as soln grid)
    {
        const int i_p = iy;
        for ( int iw = A_inv_projection_BGN; iw < A_inv_projection_BGN + A_inv_projection.length; iw++ )  // iw goes through the grid index on drvt grid
        {
            int i_d = iw * drvt_grid.Ly_pad + iy;
            ns_type::cuda_precision v_d = eta * A_inv_projection( iw - A_inv_projection_BGN ) * buffer[ i_p ];
            drvt_to_rths <bool_extra> ( v_d, i_d, P, P_extra, R, R_extra );
            // [2023/07/18]
            // NOTE: "drvt_to_rths ()" should be called at the same level where "i_d" is calculated.
        }
    }
} // cuda_secure_bdry_x ()


template<bool bool_extra>
void cuda_Class_Grid::kernel_launch_cuda_secure_bdry_x ( int N_block , int N_thread , char c_LR )
{
    ns_type::cuda_precision * S = this->Map_pntr_soln.at('x')->ptr;

    ns_type::cuda_precision * R       = this->Vec_rths.at(0).ptr;
    ns_type::cuda_precision * P       = this->Vec_prmt.at(0).ptr;

    ns_type::cuda_precision * R_extra = nullptr;
    ns_type::cuda_precision * P_extra = nullptr;

    if ( bool_extra ) 
    {
        // [2023/07/17]
        // NOTE: It would be great if we could "static assert" here to check which grid is calling.
        //       Even better, if we have a member variable "bool_normal" of the grid class that is 
        //       available at compile time, we could use that variable for the "if test" instead. 
        //       Moreover, instead of having 'y' as part of the function name, maybe we can change
        //       it to a template parameter.

        R       = this->Vec_rths.at(1).ptr;  // 0 corresponds to 'y'
        R_extra = this->Vec_rths.at(0).ptr;  // 1 corresponds to 'x', which needs extra work

        P       = this->Vec_prmt.at(0).ptr;  // lambda
        P_extra = this->Vec_prmt.at(1).ptr;  // 2 * mu
    }
    
    cuda_Struct_Grid & drvt_grid = this->struct_grid;
    cuda_Struct_Grid & soln_grid = Map_pntr_grid.at('x')->struct_grid;

    auto projection = this->Map_projection_shallow_copy.at({'x',c_LR});
    auto A_inv_projection = this->Map_A_inv_projection_shallow_copy.at({'x',c_LR});

    int projection_BGN = -1<<30;
    if ( c_LR == 'L' ) { projection_BGN = 0;                                      } 
    if ( c_LR == 'R' ) { projection_BGN = soln_grid.G_size_x - projection.length; }

    int A_inv_projection_BGN = -1<<30;
    if ( c_LR == 'L' ) { A_inv_projection_BGN = 0;                                            } 
    if ( c_LR == 'R' ) { A_inv_projection_BGN = drvt_grid.G_size_x - A_inv_projection.length; }

    ns_type::cuda_precision eta = -1<<30;
    if ( c_LR == 'L' ) { eta =   1.; } 
    if ( c_LR == 'R' ) { eta = - 1.; }  

    // this->Map_buffer.at({'x',c_LR}).memset_zero ();
    ns_type::cuda_precision * buffer = this->Map_buffer.at({'x',c_LR}).ptr;

    if ( N_block  == -1 ) { N_block  = 1;  }
    if ( N_thread == -1 ) { N_thread = 32; }

    cudaStream_t stream = 0;  // 0 is the default stream
    if ( bool_stream ) { stream = c_LR == 'L' ? this->stream_dx_L : this->stream_dx_R; }

    cuda_secure_bdry_x <bool_extra> <<< N_block , N_thread , 0 , stream >>> 
        ( soln_grid , projection , projection_BGN , drvt_grid, 
          A_inv_projection, A_inv_projection_BGN, eta, 
          S, P, P_extra, R, R_extra, buffer );
}